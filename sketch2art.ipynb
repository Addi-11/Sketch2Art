{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sketch2art.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNERcWfuhdRP1pIiPRsyLeh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Addi-11/Sketch2Art/blob/master/sketch2art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jtKMJlN8IT9"
      },
      "source": [
        "* Train a pix2pix network\n",
        "* Tain a UST\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPDk5hWm_8ob"
      },
      "source": [
        "# pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST1Ts-zN4ipt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgZtHlNq-xNh"
      },
      "source": [
        "# utilities\n",
        "def load(image):\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "    w = tf.shape(image)[1]\n",
        "\n",
        "    w = w//2\n",
        "    real_image = image[:,:w, :]\n",
        "    input_image = image[:, w:, :]\n",
        "\n",
        "    input_image = tf.cast(input_image, tf.float32)\n",
        "    real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "def resize(input_image, real_image, height, width):\n",
        "    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "def normalize(input_image, real_image):\n",
        "    input_image = (input_image / 127.5) - 1\n",
        "    real_image = (real_image / 127.5) - 1\n",
        "\n",
        "    return input_image, real_image\n",
        "\n",
        "def random_jitter(input_image, real_image):\n",
        "    input_image, real_image = resize(input_image, real_image, 286,286)\n",
        "    input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        # random mirroring\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "    return input_image, real_image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_O3e_SdY_9Q"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PyKR3JYlu5-"
      },
      "source": [
        "class Generator():\n",
        "    self.LAMBDA = 100\n",
        "    self.optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "    def __init__(self):\n",
        "        inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
        "\n",
        "        down_stack = [\n",
        "            downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "            downsample(128, 4), # (bs, 64, 64, 128)\n",
        "            downsample(256, 4), # (bs, 32, 32, 256)\n",
        "            downsample(512, 4), # (bs, 16, 16, 512)\n",
        "            downsample(512, 4), # (bs, 8, 8, 512)\n",
        "            downsample(512, 4), # (bs, 4, 4, 512)\n",
        "            downsample(512, 4), # (bs, 2, 2, 512)\n",
        "            downsample(512, 4), # (bs, 1, 1, 512)\n",
        "        ]\n",
        "\n",
        "        up_stack = [\n",
        "            upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "            upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "            upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "            upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "            upsample(256, 4), # (bs, 32, 32, 512)\n",
        "            upsample(128, 4), # (bs, 64, 64, 256)\n",
        "            upsample(64, 4), # (bs, 128, 128, 128)\n",
        "        ]\n",
        "\n",
        "        initializer = tf.random_normal_initializer(0., 0.02)\n",
        "        last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                                strides=2,\n",
        "                                                padding='same',\n",
        "                                                kernel_initializer=initializer,\n",
        "                                                activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "        x = inputs\n",
        "\n",
        "        # Downsampling through the model\n",
        "        skips = []\n",
        "        for down in down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips = reversed(skips[:-1])\n",
        "\n",
        "        # Upsampling and establishing the skip connections\n",
        "        for up, skip in zip(up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "        x = last(x)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    def loss(self, disc_generated_output, gen_output, target):\n",
        "        loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "        # mean absolute error\n",
        "        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "        total_gen_loss = gan_loss + (self.LAMBDA * l1_loss)\n",
        "\n",
        "        return total_gen_loss, gan_loss, l1_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzYY3DcTostJ"
      },
      "source": [
        "class Discriminator():\n",
        "    def __init__():\n",
        "        initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "        tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "        x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "        down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "        down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "        down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "        zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "        conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "        batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "        leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "        zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "        last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                        kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "        return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "        def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "            loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "            real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "            generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "            total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "            return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWncH7Ggl4Qt"
      },
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}